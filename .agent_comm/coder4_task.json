{
  "agent_id": "coder4",
  "task_id": "task_3",
  "files": [
    {
      "filename": "user_study_platform.py",
      "purpose": "Integration layer for Prolific recruitment and Qualtrics survey data collection",
      "priority": "medium",
      "dependencies": [
        "requests",
        "pandas",
        "json",
        "datetime"
      ],
      "key_functions": [
        "create_study_sessions",
        "assign_conditions",
        "collect_responses",
        "export_study_data"
      ],
      "estimated_lines": 250,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "SelfAssessingAI_Transparency_System",
    "project_type": "other",
    "description": "A human-AI collaboration system that implements self-assessing AI models to communicate their own strengths and weaknesses transparently. The system uses a Random Forest classifier for income prediction and a decision tree trained on model mistakes to provide interpretable explanations of AI limitations, enabling better trust calibration in human-AI teams.",
    "key_algorithms": [
      "Random Forest Classification",
      "Decision Tree for Error Prediction",
      "LIME-inspired Self-Assessment",
      "Trust Calibration Mechanisms",
      "Human-AI Interaction Protocols"
    ],
    "main_libraries": [
      "scikit-learn",
      "pandas",
      "numpy",
      "matplotlib",
      "seaborn",
      "flask",
      "sqlalchemy",
      "plotly",
      "dash",
      "jupyter",
      "qualtrics-api",
      "prolific-api"
    ]
  },
  "paper_content": "PDF: cs.HC_2508.09033v1_Beyond-Predictions-A-Study-of-AI-Strength-and-Wea.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nBeyond Predictions: A Study of AI Strength and\nWeakness Transparency Communication on\nHuman-AI Collaboration\nTina Behzad1\u22c6[0009\u22120009\u22121157\u22129082], Nikolos Gurney2[0000\u22120003\u22123479\u22122037], Ning\nWang2[0009\u22120004\u22127037\u22129260], and David V. Pynadath3[0000\u22120003\u22122452\u22124733]\n1Stony Brook University, Stony Brook, NY tbehzad@cs.stonybrook.edu\n2Insitute for Creative Technologies, University of Southern California, Los Angeles,\nCA{gurney, nwang}@ict.usc.edu\n3Rice University, Houston, TX\npynadath@rice.edu\nAbstract. The promise of human-AI teaming lies in humans and AI\nworking together to achieve performance levels neither could accomplish\nalone. Effective communication between AI and humans is crucial for\nteamwork, enabling users to efficiently benefit from AI assistance. This\npaper investigates how AI communication impacts human-AI team per-\nformance. We examine AI explanations that convey an awareness of its\nstrengths and limitations. To achieve this, we train a decision tree on\nthe model\u2019s mistakes, allowing it to recognize and explain where and\nwhy it might err. Through a user study on an income prediction task,\nwe assess the impact of varying levels of information and explanations\nabout AI predictions. Our results show that AI performance insights\nenhance task performance, and conveying AI awareness of its strengths\nand weaknesses improves trust calibration. These findings highlight the\nimportance of considering how information delivery influences user trust\nand reliance in AI-assisted decision-making.\nKeywords: AIExplainability \u00b7Decision-MakingSupport \u00b7Transparency\nin AI \u00b7Trust in AI.\n1 Introduction\nWith recent advancements in the quality and accessibility of Artificial Intelli-\ngence (AI), these systems are becoming increasingly integrated into society. In\nresponse, policymakers and practitioners emphasize the need for greater human\noversightinAI-drivendecision-making[30].Thisshiftnecessitateshuman-AIcol-\nlaboration, where individuals must review AI recommendations and ultimately\n\u22c6Work done while interning at the Institute for Creative Technologies, University of\nSouthern California.\nThe final authenticated version will be published in Lecture Notes in Computer\nScience (LNCS), Springer.arXiv:2508.09033v1  [cs.HC]  12 Aug 2025\n\n--- Page 2 ---\nmake the final decision. In such scenarios, human\u2019s trust in their AI decision-aid\nbecomes critical for the team\u2019s success [13]. The human decision-makers need\nto know when to trust or distrust an AI model\u2019s recommendations. Decades of\nresearch on this topic yielded complex insights into humans\u2019 inclination to trust\nalgorithms [51] and the problematic disuse or overuse of automation [22]. Nu-\nmerous studies have demonstrated that individuals frequently avoid relying on\ndecision-making systems in various scenarios [4,49]. However, it has also been\nshown that in many situations, people do prefer algorithms, making overreliance\na sensible worry [25].\nResearch on trust calibration in AI can be broadly categorized into two main\napproaches. The first approach emphasizes explainability, suggesting that mak-\ningblack-boxmodelsmoreinterpretablewillhelpuserscalibratetheirtrustinAI\nsystems [49,35]. However, several recent empirical studies have found little evi-\ndence that higher explainability significantly impacts users\u2019 willingness to trust\nmachine learning models [6,19]. The second approach advocates for providing\nhigh-level information about the AI system, such as its accuracy, to help users\nadjust their trust levels accordingly [26]. However, presenting high-performance\nmetrics uncritically can sometimes lead to overreliance, where users place exces-\nsive trust in AI recommendations [21].\nThese findings highlight the level of information provided to the user has\nsignificant impact on user\u2019s trust [30]. Overwhelming users with too much in-\nformation about the model leads to user following both correct and incorrect\ndecisions more often [41] while the correct level of information is proven to be\nhelpful [8]. The AI needs to communicate its own strength and weakness. By\nacknowledging its limitations, the AI can provide high-level explanations for its\ndecisions, making it easier for humans to understand and verify its reasoning.\nIn this study, we examine how varying levels of information about an AI sys-\ntem\u2019sperformance,reflectingdifferentdegreesofawarenessofitsownlimitations,\naffect human-AI collaboration in terms of performance, trust, and understand-\ning. To investigate this, we develop a self-assessing AI model, drawing inspi-\nration from explainability methods such as Local Interpretable Model-agnostic\nExplanations (LIME) [35]. We then conduct an empirical evaluation with 272\nparticipants recruited through Prolific, testing our hypothesis on whether and\nhow different levels of information and feedback impact users\u2019 trust and decision-\nmaking performance over an income prediction task. Our results show that pro-\nviding any level of information on AI\u2019s performance improves overall task perfor-\nmance compared to having no feedback. This finding aligns with prior research\nwhich suggests performance insights can enhance human-AI collaboration [20].\nRegarding trust calibration, our findings indicate that providing AI\u2019s confidence\nin its decisions or awareness of its strengths and weaknesses, both enable users to\nbetter discern when to trust the model\u2019s predictions. Moreover, conveying AI\u2019s\nawareness compared to confidence appears to slightly enhance users\u2019 ability to\ncalibrate their trust more effectively.\nThese findings highlight the importance of carefully designing the way AI\ncommunicates performance information to users. Given the diverse and some-\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 3 ---\ntimes contradictory research on what factors influence trust in AI, it is crucial\nto investigate the granularity of information provided and how it is conveyed.\nOur work underscores the need for further research on the impact of different\nlevels and formats of feedback, helping to refine human-AI interaction strategies\nand ensure users can develop appropriately calibrated trust in AI systems.\n2 Related Work\nAdvances in Machine Learning (ML) and ML-based AI in recent years have en-\nabled these systems to exceed human-level performance in making predictions.\nOne particularly important use case of machine learning is supporting decisions.\nDecision support systems (DSS) have undergone several evolutionary waves, and\nthe integration of machine learning promises to drive another significant leap\nforward [48]. However, despite this progress, algorithmic aversion, where people\ndistrust and avoid using algorithms for decision-making, especially after observ-\ning them make mistakes [10], has become a major barrier to fully leveraging their\ncapabilities. Research shows that users often resist incorporating algorithms into\ndecision-making across various domains, including critical tasks such as aiding\nprofessionals in making medical recommendations [33,40], receiving medical [25]\nor financial [12] advice, and employee selection [9]. This resistance even extends\nto lower-stakes tasks, such as receiving joke recommendations [49], raising con-\ncerns about the feasibility of joint human-algorithm decision-making in practice\n[5].\nPrevious research has tried to understand and address the factors contribut-\ning to this reluctance and distrust. A review of studies on the topic between\n1950 and 2018 identified key themes influencing algorithmic aversion, including\nexpectations and expertise, decision autonomy, incentivization, cognitive com-\npatibility, and divergent rationalities [5].\nHowever, while some users resist AI-based decisions, others display overre-\nliance on AI, accepting its recommendations even when they are incorrect. This\noverreliance could be caused by human decision-making biases, such as automa-\ntion bias [32,25] and confirmation bias [26]. Such overreliance is also influenced\nby various human factors, e.g., individual differences [30,32,7], and situational\nfactors, like ordering effects, such as the sequence in which AI errors occur[28,\n29].\nThis often occurs when users struggle to assess whether\u2014and to what ex-\ntent\u2014they should trust the AI [30], highlighting that achieving high trust as a\nsolutiontoalgorithmicaversionshouldnotbetheultimategoal.Insufficienttrust\nmayleaduserstorejectAIassistance,evenwhenitcouldimproveoutcomes(dis-\ntrust/aversion) [24]. Conversely, excessive trust can cause users to perform worse\nthan either the AI or human alone [2]. An appropriate level of trust\u2014calibrated\ntrust\u2014 is essential [18]. As a result, trust calibration has emerged as a critical\narea of research.\nTrust calibration has been extensively studied in automation tasks [15,23,\n27] and more recently in AI-assisted decision-making [21,36,50]. Several studies\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 4 ---\nhave examined the impact of accuracy information on trust, showing that users\ntend to increase their trust in AI when high accuracy indicators are displayed\n[50,34]. Others have focused on the role of explanations in shaping trust, ar-\nguing that the black-box nature of AI presents a barrier to adoption [3,24,47].\nMore recently, Daehwan Ahn et al. [1] integrated these two research streams to\nexamine their relationship. Their findings indicate that while both factors had\nmodest effects on participants\u2019 performance, interpretability did not lead to a ro-\nbust improvement in trust, whereas providing accuracy information significantly\nincreased trust. Building on previous research highlighting the significance and\nimpact of outcome feedback, in this paper, we investigate how varying levels of\nfeedback influence human-AI collaboration.\n3 Self-assessing AI\nExploring different levels of outcome feedback, our goal was to develop a model\nthat is aware of its strengths and limitations and capable of identifying predic-\ntions where it might be incorrect. A straightforward and widely used approach to\nachievethisisthroughconfidencescores,whichprovideameasureofuncertainty.\nThe method for calculating these scores varies depending on the model type.\nFor probabilistic models such as Naive Bayes and Logistic Regression, confidence\nscores are directly obtained as class probabilities produced by the model. In\nensemble models like Random Forests or Gradient Boosting, confidence scores\ncanbecalculatedastheproportionoftrees/modelsvotingforaspecificclass.For\nmore complex models, such as Neural Networks, the confidence score is typically\nderived from the activation function of the output layer, such as the softmax\nfunction, which provides a probability distribution over classes[45].\nConfidence scores have been explored as a way to provide outcome feedback\nto users. Zhang et al. found that confidence scores help calibrate users\u2019 trust in\nAI models [51]. Similarly, Rechkemmer et al. demonstrated that a model\u2019s con-\nfidence level significantly influences users\u2019 perception of its accuracy, impacting\nboth their willingness to follow its predictions and their self-reported trust in\nthe model [34].\nWhile confidence scores shows promise in guiding human-AI interaction, they\ndo not constitute awareness\u2014they are simply mathematical outputs derived\nfrom the model\u2019s internal calculations. These scores do not imply that the model\nknows why it might be wrong; rather, they reflect the model\u2019s certainty based\non its training data without any underlying reasoning ability. To address these\nlimitations, alternative approaches have been explored. One promising approach\nis evidential learning [39,38]. The evidential learning approach learns a gener-\native model to create out-of distribution samples so that the classifier can be\nexplicitly taught the input regions it should be uncertain about [38]. However,\nthis approach introduces additional complexities in the training process [42].\nAnother important consideration is how humans process probabilistic in-\nformation. Research has shown that people often struggle with probabilistic\nreasoning, falling into errors such as the base-rate fallacy [17,44,43]. Studies\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 5 ---\nsuggest that alternative, non-probabilistic representations of uncertainty or con-\nfidence can lead to improved trust calibration [16,27]. Building on these find-\nings and inspired by similar approaches in explainable AI, such as Local Inter-\npretable Model-agnostic Explanations (LIME) [35], which train simpler, more\ninterpretable models to approximate the behavior of complex models, we adopt\na similar strategy. Specifically, we train a decision tree, a highly interpretable\nmodel, on the original dataset but with labels indicating whether the predictions\nof the original model were correct or incorrect. This allows us to create a model\nthat not only identifies where the complex model is likely to make mistakes but\nalso explains why, by analyzing how different features contribute to these er-\nrors. Additionally, the decision tree can predict for new data points whether the\noriginal model is likely to make a mistake.\n3.1 Defining the Task\nResearchers have argued that results and recommendations for human-centered\ninteraction with AI may vary depending on the context of use [24]. For our\nstudy, we selected an income prediction task, which has been used in prior user\nstudies on decision support systems [46]. This scenario was chosen based on the\ncriteria outlined by Leichtmann et al. [24]. Income prediction is highly rele-\nvant, as it plays a crucial role in hiring, financial assessments, and social policy.\nUnlike specialized fields like medical diagnosis, income-related decisions do not\nrequire expert knowledge , making it easier to recruit participants from di-\nverse backgrounds. Additionally, people regularly assess income-related factors\nin everyday life, ensuring the task is close to participants\u2019 reality . Finally,\nconcerns surrounding income equity and fairness make this a topic of significant\npublic interest .\nWeusedtheAmericanCommunitySurvey(ACS)PublicUseMicrodataSam-\nple (PUMS) dataset [11], which covers multiple years and all states across the\nUnited States. It supports five different prediction tasks, including income pre-\ndiction. For this study, we restricted the dataset to individuals from California\nin the year 2018, resulting in 196,665individuals.\nWe chose our original model to be a Random Forest classifier, trained using\nthe scikit-learn library [31] with default hyperparameters. We used 70% of the\ndata for training, achieving an accuracy of 80% on the test set. To create a\nself-assessing system, we trained a decision tree on the same training dataset\nbut replaced the original labels with binary indicators of whether the original\nmodel\u2019s predictions were correct. We selected a decision tree model for three key\nreasons. First, its structure is inherently interpretable, allowing us to trace and\nexplain each decision path. Second, because decision trees align with everyday\nreasoning, a lot of people without any background can understand the model\u2019s\nlogic, and we can tailor the depth of information displayed to the user. Third,\nthe tree\u2019s natural grouping of data points into branches creates clusters that\nwe leverage later in our study design (see section 4.1). Since the original model\nhad an accuracy of 90% on the training data, the resulting dataset was highly\nimbalanced. To address this, we balanced the data to contain an equal number\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 6 ---\nof correct and incorrect labels before training the decision tree. After tuning\nhyperparameters, the self-assessing model achieved a 66% accuracy which was\nthe best we could get under these conditions. We call this tree the flaw decision\ntree.\n4 Methods\n4.1 Study Design\nOur main hypotheses when designing the experiments were:\nH1Communicating awareness of the model\u2019s weakness and strength can improve\ntask performance.\nH2Communicating awareness of model\u2019s weakness and strength can help human\nteammates calibrate trust in AI.\nH3Communicating awareness of the model\u2019s weakness and strength can help\nhuman teammates understand AI better.\nTo evaluate this, we designed an experiment consisting of four treatment con-\nditions and a control group. The task required participants to predict whether\nan individual\u2019s income would be above or below $80K after reviewing relevant\nattributes. All groups were presented with the individual\u2019s information in a tab-\nular format, as shown in Figure 4. The treatment groups received varying levels\nof assistance and feedback from an AI teammate (the trained random forest\npredictor from section 3), while the control group (we also call this group No\nHelper through the text) completed the task without AI support. This setup\nallowed us to assess participants\u2019 performance both without and with different\nlevels of AI assistance and outcome feedback.\nThefourtreatmentgroupswereasfollows(illustratedinFigure4inappendix\nB):\nG1 Decision-Only :SawtheAIhelper\u2019spredictionforeachindividualalongside\nthe individual\u2019s information.\nG2 Decision + 80% Exp : Received the same information as G1, but before\nstarting, they were informed that the AI\u2019s accuracy is 80% and were given\nan explanation of what this means.\nG3 Decision + Confidence Exp : In addition to seeing the AI\u2019s prediction\nand the individual\u2019s attributes, they were also shown the model\u2019s confidence\nin its prediction.\nG4 Decision + self-assessing Exp : Along with the AI\u2019s prediction and the\nindividual\u2019s attributes, they were provided with information on how well the\nmodel performs for similar data points and the overall performance (80%).\nWe selected 50 data points from the full test dataset described in section\n3.1, allocating 10 for the training phase and 40 for the main task. To ensure\nTo have a better understanding of how the tree looks, refer to Figure 2.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 7 ---\nconsistency, we selected instances in a way that the AI\u2019s predictions maintained\nthe reported 80% accuracy (2 of 10 incorrect predictions in training and 8 out\nof 40 predictions in the main task).\nFor Group 3, the confidence score corresponded to the probability of the\npredicted class (income level above $80K). As stated in the scikit-learn docu-\nmentation, this score is calculated as the mean predicted class probabilities of\nthe trees in the forest . For Group 4, we use the self-assessing model described\nin section 3. Initially, we considered presenting participants with the entire flaw\ndecision tree, highlighting where each individual\u2019s data point falls within the\ntree or displaying the full root-to-leaf decision path. However, we determined\nthat this approach could overwhelm participants with too much information,\nparticularly for those unfamiliar with tree structures. We chose to use the flaw\ndecision tree to categorize data points into groups, where each leaf node repre-\nsents a group of similar individuals based on whether the original model predicts\nthem correctly. For each group, we report the fraction of individuals that were\npredicted correctly by the original model. We present this information as \"AI\u2019s\naccuracy for similar individuals\", alongside the general statement that the AI\u2019s\noverall accuracy is 80% for each prediction.\nWhen selecting the 50 data points from the test dataset, we ensured that the\nadditional information provided to treatment groups 3 and 4 remained consis-\ntent. Specifically, if the confidence score displayed to Group 3 for a given point\nwas high, the AI\u2019s accuracy for similar individuals shown to Group 4 was also\nhigh, and vice versa. While the exact numerical values may differ, we ensured\nthat both metrics consistently reflected whether they were above or below the\noverall 80% accuracy threshold. Additionally, for instances where the original\nmodel made incorrect predictions, we balanced the selection of data points. Half\nof these instances were chosen so that the reported confidence or accuracy was\ninformative (i.e., lower than the overall 80% accuracy), indicating appropriate\nuncertainty. The other half was selected to be misleading, where the model was\noverconfident despite being incorrect.\nForeachoftheaforementionedconditions,afteridentifyingthesubsetofdata\ncorresponding to that specific condition, the points were randomly selected from\nthe entire test set.\n4.2 Ethical Approval\nThis study was approved by the University of Southern California Institutional\nReview Board.\n4.3 Recruitment\nWe recruited 272 (approximately 50 per condition) US-based participants using\nthe Prolific pool of study participants, where no personally identifying informa-\ntion was accessible. They were informed that the study would take place online,\nThe class probability of a single tree is determined by the fraction of samples of the\nsame class in a leaf.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 8 ---\nmight involve working with an AI, last approximately fifteen minutes, pay $4\nplus a bonus of up to $4, and was open to adult US citizens (a funding con-\nstraint). The average participant was 37 years old, most of whom ( n= 177)\nreported being female. The modal reported level of education was a bachelor\u2019s\ndegree ( n= 102); the next most common was a high school diploma or equiv-\nalent ( n= 74). The majority of participants ( n= 185) self identified as white.\nThe median completion time was 889.5 seconds, however, the completion time\ndata are characterized by a right skew ( mean = 1025 .2seconds).\n4.4 Procedure\nWe collected data in two batches: the four treatment conditions and the control\ngroup, which did not have an AI helper. After accepting the task on Prolific, the\nparticipant was redirected to a survey-based platform (Qualtrics) where they\nsigned the consent form and agreed to participate in the study. The survey\nsoftware randomized treatment condition participants into a study arm. Verifi-\ncation of their Prolific ID advanced participants to an introduction page that\nintroduced them to the task and, if in a treatment condition, how and what\nthe AI helper would communicate to them. Next, they progressed to a training\nphase in which they were allowed to do 10 practice tasks that did not impact\ntheir bonus payment. Participants in treatment conditions received the same\nhelp from the AI that they eventually did during the incentivized portion of\nthe study. The training phase ended with a report telling participants how well\nthey did in the classification task and reminding them that the following 40 clas-\nsifications tasks were bonus-eligible. In both the training and the actual task,\nusers received feedback on whether or not they made the correct decision after\neach decision. At the end of the 40 tasks, participants were informed of their\nperformance and the bonus amount they earned.\nParticipants next completed a set of self-report questions (see appendix D\nfor the complete set of questions). The Generalized Attitudes Towards AI Scale\n[37] followed the self-report. Finally, participants completed the demographics\nportionofthestudy,afterwhichtheywereredirectedbacktoProlifictocomplete\nthe platform requirements for payment.\n4.5 Measures\nTask Performance : We measure performance as the proportion of correct final\ndecisions, computed as the number of accurate predictions divided by the total\nnumber of predictions (40).\nCompliance : Compliance is defined as instances where the user\u2019s final decision\nis aligned with the AI\u2019s recommendation, serving as a proxy for trust in the\nAI\u2019s decision. To further investigate users\u2019 ability to calibrate their trust, we\ndifferentiate compliance rates based on the correctness and confidence of the\nAI\u2019s predictions. Specifically, we distinguish between cases where the AI was (i)\ncorrect, (ii) incorrect and overconfident, and (iii) incorrect with an appropriate\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 9 ---\nlevel of confidence. This differentiation allows us to assess whether users could\nappropriately override AI recommendations when the model was incorrect.\nSelf-Reported Perceptions of the AI Assistant : To evaluate whether varying lev-\nels of outcome feedback influenced participants\u2019 understanding of the AI team-\nmate, we administered a post-task questionnaire consisting of four key questions.\nParticipants rated their agreement with each statement using a slider scale from\n0 to 100. These questions were not presented to the control group. 1) The AI\nunderstands how the information in the tables relates to income levels. 2)The\nAI knows its own limitations. 3) I trusted the AI to provide useful suggestions.\n4) I am confident that I know how the AI makes its suggestions.\n5 Results\nIn the following, we present results related to our first (H1) and second (H2)\nhypotheses. Our self-reported measures on participants\u2019 understanding of the\nAI helper (H3) showed no significant differences across groups. Due to space\nconstraints, a more detailed discussion of these findings has been moved to the\nappendix.\n5.1 Task Performance\nWe start by looking at task performance. Figure 1 shows participants\u2019 scores\nacross the training phase and for the actual task. The scores were scaled from\nto 100 for illustration (exact mean value can be found in table 1 in appendix\nC). To better interpret the results we fitted a linear regression model(presented\nFig.1: Box plot illustrating training scores (blue) and task scores (orange) for\neach group.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 10 ---\nin Appendix C, Table 2). Results suggest that participants with input from an\nAI helper generally did better than participants in the No Helper , i.e., control,\ncondition (see Table 2 column (1) for the values used in the following inter-\npretation). The average score of the No Helper participants was 0.780, or just\nover 31 correct classifications. Participants in the Decision-Only andDecision +\n80% Exp conditions did better (their average scores were 0.780 + 0 .010 = 0 .790\nand 0.780 + 0 .016 = 0 .796, respectively), although not significantly. Participants\nin the other two conditions Decision + Confidence Exp andDecision + self-\nassessing Exp did significantly better than the control participants, averaging\n0.780 + 0 .045 = 0 .825and 0.780 + 0 .035 = 0 .815, respectively\u2014equating to\nroughly 33 correct classifications.\nThe effects remain roughly the same when adding control variables to the\nbasic model (see Table 2 column (2) for the values used in the following inter-\npretation). We reached this model through iteratively specifying models using\nthe various demographic and control variables (e.g, Training Score ,Age,Sex,\nEducation , etc.) and using a \u03c72test to see if the reduction in the residual sum\nof squares was justified by the higher complexity of a model with additional\nvariables. Generally speaking, participants that did better during the training\nphase did better during the incentivized task, and each additional year of age\nwas associated with a small but significant decrease in score.\n5.2 Compliance\nAlthough predicting participants\u2019 overall performance (their score) in the task is\nvaluable, it is arguably more important to understand how calibrated they are\nwhen they should heed or ignore the helper\u2019s advice. Well-calibrated compliance\nis not only correlated with performance but also opinions of the help [14]. For\nthis, we use logistic regression models and, having established the importance of\nAgeandTraining Score in our previous efforts, only discuss the fully specified\nmodels (see Table 3). Note that we do not include the data from the partici-\npants in the No Helper treatment condition as they were not choosing whether\nto comply with the recommendation of an AI helper. Thus, the Decision-Only\ntreatment condition serves as the \u201ccontrol\u201d condition.\nOur results show that relative to the Decision-Only , participants in each of\nthe other three helper conditions were significantly more likely to comply. When\nwe look at overall compliance, meaning over the entire task regardless of whether\nor not the helper was correct, the strongest impact is observed in Decision +\nConfidence Exp condition which was associated with a 0.305 high log odds of\ncomplying (column (1), Table 3). In other words, for these participants, there\nwas a 35% increase in the odds of complying ( (e0.305\u22121)\u2217100). However, when\nwe look at compliance when the helper was correct (column (3), Table 3), we see\nthe most substantial effect appears in the Decision + self-assessing Exp group\nincrease in the odds of complying, followed by Decision + Confidence Exp .\nWe further examine the odds of compliance when the AI is incorrect and\noverconfident versus incorrect with the right level of confidence (see Table 4).\nParticipantsinthe Decision + Confidence Exp andDecision + self-assessing Exp\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 11 ---\nconditions are significantly more likely to comply when the AI is incorrect but\noverconfident and less likely to comply when the AI is incorrect but calibrated in\nits confidence. While the differences between the two groups are not statistically\nsignificantineitherscenario,weobserveslightlylowercomplianceinthe Decision\n+ self-assessing Exp rate when the AI is overconfident (and similarly for the\nright confidence case), suggesting that participants in the awareness condition\ndemonstrated better trust calibration in AI\u2019s decisions.\n6 Discussion\nIn this paper, we explored the design of an AI decision aid that generates expla-\nnationshighlightingitsstrengthsandweaknessesinpredictingincomelevels.Our\nhypothesis posited that providing concise, awareness-based insights\u2014reflecting\na deeper understanding of the model\u2019s limitations\u2014could enhance task perfor-\nmance, trust calibration, and users\u2019 comprehension of their AI teammate. To\nevaluate this, we conducted a user study with 272 participants, including a con-\ntrol group and four treatment conditions, each offering varying levels of assis-\ntance and feedback from the AI teammate. Our results demonstrate that provid-\ning AI performance information enhances task performance and that conveying\nAI\u2019s awareness of its strengths and weaknesses helps users calibrate their trust\nin its decisions slightly better compared to showing only confidence levels.\nOur findings align with prior research suggesting that providing informa-\ntion about AI models helps users develop appropriate reliance on AI while also\noffering insights into how to enhance the effectiveness of this information. Over-\nreliance is particularly critical in cases where the AI is incorrect, yet users choose\nto follow its recommendations. Our results suggest that conveying AI awareness\ncan slightly reduce this blind trust more effectively than simply providing con-\nfidence levels.\nWhilewedidnotobservesignificantdifferencesacrossgroupsintermsoftheir\nunderstanding of AI, this may be due to limitations in the objective measures\nused. Future research could address this by incorporating follow-up assessments\nto more effectively evaluate participants\u2019 understanding of AI. Additionally, it\nwould be valuable to examine whether these findings replicate across different\ntasks and varying levels of AI accuracy. Insights from such studies can further\nilluminate the complex dynamics of human-AI collaboration.\n7 Acknowledgement\nResearch was sponsored by the Army Research Office and was accomplished\nunder Cooperative Agreement Number W911NF-20-2-0053. The views and con-\nclusions contained in this document are those of the authors and should not be\ninterpreted as representing the official policies, either expressed or implied, of\nthe Army Research Office or the U.S. Government. The U.S. Government is au-\nthorized to reproduce and distribute reprints for Government purposes notwith-\nstanding any copyright notation herein.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 12 ---\nReferences\n1. Ahn, D., Almaatouq, A., Gulabani, M., Hosanagar, K.: Impact of\nmodel interpretability and outcome feedback on trust in ai. In: Pro-\nceedings of the 2024 CHI Conference on Human Factors in Com-\nputing Systems. CHI \u201924, Association for Computing Machinery,\nNew York, NY, USA (2024). https://doi.org/10.1145/3613904.3642780,\nhttps://doi.org/10.1145/3613904.3642780\n2. Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M.T., Weld,\nD.: Does the whole exceed its parts? the effect of ai explanations on complementary\nteam performance. In: Proceedings of the 2021 CHI conference on human factors\nin computing systems. pp. 1\u201316 (2021)\n3. Bhatt,U.,Ravikumar,P.,etal.:Buildinghuman-machinetrustviainterpretability.\nIn: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 9919\u2013\n9920 (2019)\n4. Bigman, Y.E., Gray, K.: People are averse to machines making moral decisions.\nCognition 181, 21\u201334 (2018)\n5. Burton, J.W., Stein, M.K., Jensen, T.B.: A systematic review of algorithm aversion\nin augmented decision making. Journal of behavioral decision making 33(2), 220\u2013\n239 (2020)\n6. Cheng, H.F., Wang, R., Zhang, Z., O\u2019connell, F., Gray, T., Harper, F.M., Zhu, H.:\nExplaining decision-making algorithms through ui: Strategies to help non-expert\nstakeholders. In: Proceedings of the 2019 chi conference on human factors in com-\nputing systems. pp. 1\u201312 (2019)\n7. Chong, L., Zhang, G., Goucher-Lambert, K., Kotovsky, K., Cagan, J.: Human\nconfidence in artificial intelligence and in themselves: The evolution and impact of\nconfidence on adoption of ai advice. Computers in Human Behavior 127, 107018\n(2022)\n8. De-Arteaga, M., Fogliato, R., Chouldechova, A.: A case for humans-in-the-loop:\nDecisions in the presence of erroneous algorithmic scores. In: Proceedings of the\n2020 CHI Conference on Human Factors in Computing Systems. pp. 1\u201312 (2020)\n9. Diab, D.L., Pui, S.Y., Yankelevich, M., Highhouse, S.: Lay perceptions of selection\ndecision aids in us and non-us samples. International Journal of Selection and\nAssessment 19(2), 209\u2013216 (2011)\n10. Dietvorst, B.J., Simmons, J.P., Massey, C.: Algorithm aversion: people erroneously\navoidalgorithmsafterseeingthemerr.Journalofexperimentalpsychology:General\n144(1), 114 (2015)\n11. Ding, F., Hardt, M., Miller, J., Schmidt, L.: Retiring adult: New datasets for fair\nmachine learning. Advances in neural information processing systems 34, 6478\u2013\n6490 (2021)\n12. Eastwood, J., Snook, B., Luther, K.: What people want from their professionals:\nAttitudes toward decision-making strategies. Journal of Behavioral Decision Mak-\ning25(5), 458\u2013468 (2012)\n13. Glikson, E., Woolley, A.W.: Human trust in artificial intelligence: Review of em-\npirical research. Academy of Management Annals 14(2), 627\u2013660 (2020)\n14. Gurney, N., Pynadath, D.V., Wang, N.: My actions speak louder than your words:\nwhen user behavior predicts their beliefs about agents\u2019 attributes. In: International\nConference on Human-Computer Interaction. pp. 232\u2013248. Springer (2023)\n15. Helldin, T., Falkman, G., Riveiro, M., Davidsson, S.: Presenting sys-\ntem uncertainty in automotive uis for supporting trust calibration in\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 13 ---\nautonomous driving. In: Proceedings of the 5th International Confer-\nence on Automotive User Interfaces and Interactive Vehicular Applica-\ntions. p. 210\u2013217. AutomotiveUI \u201913, Association for Computing Machin-\nery, New York, NY, USA (2013). https://doi.org/10.1145/2516540.2516554,\nhttps://doi.org/10.1145/2516540.2516554\n16. Helldin, T., Falkman, G., Riveiro, M., Davidsson, S.: Presenting system uncer-\ntainty in automotive uis for supporting trust calibration in autonomous driving.\nIn: Proceedings of the 5th international conference on automotive user interfaces\nand interactive vehicular applications. pp. 210\u2013217 (2013)\n17. Kahneman, D., Tversky, A.: Variants of uncertainty. Cognition 11(2), 143\u2013157\n(1982)\n18. Kraus, J., Scholz, D., Stiegemeier, D., Baumann, M.: The more you know: trust\ndynamics and calibration in highly automated driving and the effects of take-\novers, system malfunction, and system transparency. Human factors 62(5), 718\u2013\n736 (2020)\n19. Kunkel, J., Donkers, T., Michael, L., Barbu, C.M., Ziegler, J.: Let me explain:\nImpact of personal and impersonal explanations on trust in recommender systems.\nIn:Proceedingsofthe2019CHIconferenceonhumanfactorsincomputingsystems.\npp. 1\u201312 (2019)\n20. Lai, V., Liu, H., Tan, C.: \" why is\u2019 chicago\u2019deceptive?\" towards building model-\ndriventutorialsforhumans.In:Proceedingsofthe2020CHIConferenceonHuman\nFactors in Computing Systems. pp. 1\u201313 (2020)\n21. Lai, V., Tan, C.: On human predictions with explanations and predictions of ma-\nchine learning models: A case study on deception detection. In: Proceedings of the\nconference on fairness, accountability, and transparency. pp. 29\u201338 (2019)\n22. Lee, J.D.: Review of a pivotal human factors article:\u201chumans and automation: use,\nmisuse, disuse, abuse\u201d. Human Factors 50(3), 404\u2013410 (2008)\n23. Lee, J.D., See, K.A.: Trust in automation: Designing for appropriate reliance. Hu-\nman factors 46(1), 50\u201380 (2004)\n24. Leichtmann, B., Humer, C., Hinterreiter, A., Streit, M., Mara, M.: Effects of ex-\nplainable artificial intelligence on trust and human behavior in a high-risk decision\ntask. Computers in Human Behavior 139, 107539 (2023)\n25. Logg, J.M., Minson, J.A., Moore, D.A.: Algorithm appreciation: People prefer al-\ngorithmic to human judgment. Organizational Behavior and Human Decision Pro-\ncesses 151, 90\u2013103 (2019)\n26. Lu, Z., Yin, M.: Human reliance on machine learning models when performance\nfeedback is limited: Heuristics and risks. In: Proceedings of the 2021 CHI Confer-\nence on Human Factors in Computing Systems. pp. 1\u201316 (2021)\n27. McGuirl, J.M., Sarter, N.B.: Supporting trust calibration and the effective use of\ndecision aids by presenting dynamic system confidence information. Human factors\n48(4), 656\u2013665 (2006)\n28. Nourani,M.,King,J.,Ragan,E.:Theroleofdomainexpertiseinusertrustandthe\nimpact of first impressions with intelligent systems. In: Proceedings of the AAAI\nConference on Human Computation and Crowdsourcing. vol. 8, pp. 112\u2013121 (2020)\n29. Nourani, M., Roy, C., Block, J.E., Honeycutt, D.R., Rahman, T., Ragan, E.,\nGogate, V.: Anchoring bias affects mental model formation and user reliance in\nexplainable ai systems. In: Proceedings of the 26th International Conference on\nIntelligent User Interfaces. pp. 340\u2013350 (2021)\n30. Passi, S., Vorvoreanu, M.: Overreliance on ai literature review. Microsoft Research\n(2022)\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 14 ---\n31. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,\nBlondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,\nCournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine\nlearning in Python. Journal of Machine Learning Research 12, 2825\u20132830 (2011)\n32. Pop, V.L., Shrewsbury, A., Durso, F.T.: Individual differences in the calibration\nof trust in automation. Human factors 57(4), 545\u2013556 (2015)\n33. Promberger, M., Baron, J.: Do patients trust computers? Journal of Behavioral\nDecision Making 19(5), 455\u2013468 (2006)\n34. Rechkemmer, A., Yin, M.: When confidence meets accuracy: Exploring the effects\nof multiple performance indicators on trust in machine learning models. In: Pro-\nceedings of the 2022 chi conference on human factors in computing systems. pp.\n1\u201314 (2022)\n35. Ribeiro, M.T., Singh, S., Guestrin, C.: \" why should i trust you?\" explaining the\npredictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining. pp. 1135\u20131144 (2016)\n36. Schaffer, J., O\u2019Donovan, J., Michaelis, J., Raglin, A., H\u00f6llerer, T.: I can do better\nthan your ai: expertise and explanations. In: Proceedings of the 24th International\nConference on Intelligent User Interfaces. pp. 240\u2013251 (2019)\n37. Schepman, A., Rodway, P.: Initial validation of the general attitudes to-\nwards artificial intelligence scale. Computers in Human Behavior Reports\n1, 100014 (2020). https://doi.org/https://doi.org/10.1016/j.chbr.2020.100014,\nhttps://www.sciencedirect.com/science/article/pii/S2451958820300142\n38. Sensoy, M., Kaplan, L., Cerutti, F., Saleki, M.: Uncertainty-aware deep classifiers\nusing generative models. In: Proceedings of the AAAI conference on artificial in-\ntelligence. vol. 34, pp. 5620\u20135627 (2020)\n39. Sensoy, M., Kaplan, L., Kandemir, M.: Evidential deep learning to quantify classi-\nfication uncertainty. Advances in neural information processing systems 31(2018)\n40. Shaffer, V.A., Probst, C.A., Merkle, E.C., Arkes, H.R., Medow, M.A.: Why do\npatients derogate physicians who use a computer-based diagnostic support system?\nMedical Decision Making 33(1), 108\u2013118 (2013)\n41. Suresh, H., Lao, N., Liccardi, I.: Misplaced trust: Measuring the interference of\nmachine learning in human decision-making. In: Proceedings of the 12th ACM\nConference on Web Science. pp. 315\u2013324 (2020)\n42. Tomsett, R., Preece, A., Braines, D., Cerutti, F., Chakraborty, S., Srivastava,\nM., Pearson, G., Kaplan, L.: Rapid trust calibration through interpretable and\nuncertainty-aware ai. Patterns 1(4) (2020)\n43. Tversky, A., Kahneman, D.: The framing of decisions and the psychology of choice.\nscience 211(4481), 453\u2013458 (1981)\n44. Tversky, A., Kahneman, D., Slovic, P.: Judgment under uncertainty: Heuristics\nand biases. Cambridge (1982)\n45. Vemuri, N.: Scoring confidence in neural networks. University of California at\nBerkeley (2020)\n46. Vodrahalli, K., Daneshjou, R., Gerstenberg, T., Zou, J.: Do humans trust advice\nmore if it comes from ai? an analysis of human-ai interactions. In: Proceedings of\nthe 2022 AAAI/ACM Conference on AI, Ethics, and Society. pp. 763\u2013777 (2022)\n47. Wang, X., Yin, M.: Are explanations helpful? a comparative study\nof the effects of explanations in ai-assisted decision-making. In: Pro-\nceedings of the 26th International Conference on Intelligent User In-\nterfaces. p. 318\u2013328. IUI \u201921, Association for Computing Machinery,\nNew York, NY, USA (2021). https://doi.org/10.1145/3397481.3450650,\nhttps://doi.org/10.1145/3397481.3450650\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 15 ---\n48. Watson, H.J.: Preparing for the cognitive generation of decision support. MIS\nQuarterly Executive 16(3) (2017)\n49. Yeomans, M., Shah, A., Mullainathan, S., Kleinberg, J.: Making sense of recom-\nmendations. Journal of Behavioral Decision Making 32(4), 403\u2013414 (2019)\n50. Yin, M., Wortman Vaughan, J., Wallach, H.: Understanding the effect of accuracy\non trust in machine learning models. In: Proceedings of the 2019 chi conference on\nhuman factors in computing systems. pp. 1\u201312 (2019)\n51. Zhang, Y., Liao, Q.V., Bellamy, R.K.: Effect of confidence and explanation on\naccuracyandtrustcalibrationinai-assisteddecisionmaking.In:Proceedingsofthe\n2020 conference on fairness, accountability, and transparency. pp. 295\u2013305 (2020)\nA self-assessing AI\nA subset of the decision tree trained on the initial model, as described in section\n3.1, is presented in Figure 2. The full tree has a depth of 10 and consists of 517\nnodes.\nFig.2: A small selection of leaf nodes from the self-assessing decision tree trained\n(values for features are normalized).\nB Study Design\nFigure 4 shows different conditions described in section 4.1. Figure 3 shows the\ninformation provided to all participants and Figures 3a to 3d show different\nlevels of information provided to different groups.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 16 ---\nC Results\nTable 1: Mean Scores by Condition\nCondition Mean Score\nNo Helper 0.780\nDecision-Only 0.790\nDecision + 80% Exp 0.796\nDecision + Confidence Exp 0.825\nDecision + self-assessing Exp 0.815\nOur self-reported measures on participants\u2019 understanding of the AI helper\ndid not reveal any significant differences across groups (Tables 5, 6). Opiniono-\nfAIhelper 1 to 4 indicate self-reported questions described in section 4.5. The\nwillingness to pay column (Table 6, column (2)) is another self-reported metric\nwe asked from participants to evaluate the potential impact of different treat-\nment conditions on reducing algorithmic aversion. Participants were asked: If\nyou were to repeat this task but had to pay for AI suggestions from your bonus,\nwhat percentage of your bonus would you be willing to give up to receive them?\nIt is possible that with a larger sample size and more detailed questions, we\ncould gain deeper insights into how different levels of information influence users\u2019\ncomprehension of the AI system.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 17 ---\nFig.3: The information provided to all\ngroups.\n(a) Additional information shown to G2\n(b) Additional information shown to G3\n(c) Additional information shown to G4\n(d) Additional information shown to G5\nFig.4: Example of the introduction shown to all participants\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 18 ---\nTable 2: Column (1) presents a linear regression model predicting a Score (num-\nber of correct income classifications divided by total classifications). The inter-\ncept, or constant, is the No Helper condition. Column (2) adds control variables\nfor the participants\u2019 performance during the training stage and age.\nDependent variable: Score\nnCorrect \u00f7Total\n(1) (2)\nDecision-Only 0.010 0.011\n(0.017) (0.016)\nDecision + 80% Exp 0.016 0.011\n(0.017) (0.016)\nDecision + Confidence Exp 0.045\u2217\u22170.041\u2217\n(0.017) (0.016)\nDecision + self-assessing Exp 0.035\u22170.037\u2217\n(0.017) (0.016)\nTraining Score 0.038\u2217\u2217\u2217\n(0.005)\nAge \u22120.001\u2217\n(0.0004)\nConstant (No Helper) 0.780\u2217\u2217\u22170.526\u2217\u2217\u2217\n(0.012) (0.041)\nObservations 272 272\nR20.033 0.222\nAdjusted R20.018 0.204\nResidual Std. Error 0.090 (df = 267) 0.081 (df = 265)\nF Statistic 2.257 (df = 4; 267) 12.575\u2217\u2217\u2217(df = 6; 265)\nNote:\u2217p<0.05;\u2217\u2217p<0.01;\u2217\u2217\u2217p<0.001\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 19 ---\nTable 3: Logistic regression models predicting the odds of complying across dif-\nferent experimental conditions. Column (1) presents overall compliance, while\nColumns (2) and (3) distinguish between compliance when the AI was incorrect\nandcorrect,respectively.Reportedcoefficientsrepresentlog-oddsestimates,with\nstandard errors in parentheses.\nDependent variable: Odds of Complying\nOverall Helper Incorrect Helper Correct\n(1) (2) (3)\nDecision + 80% Exp 0.270\u2217\u22170.288 0.279\u2217\n(0.102) (0.162) (0.138)\nDecision + Confidence Exp 0.305\u2217\u2217\u22120.068 0.721\u2217\u2217\u2217\n(0.101) (0.152) (0.151)\nDecision + self-assessing Exp 0.268\u2217\u2217\u22120.193 0.803\u2217\u2217\u2217\n(0.100) (0.151) (0.154)\nAge \u22120.003 0.001 \u22120.006\n(0.003) (0.005) (0.005)\nTraining Score 0.098\u2217\u2217\u22120.077 0.240\u2217\u2217\u2217\n(0.032) (0.056) (0.041)\nConstant (Decision-Only) 1.412\u2217\u2217\u22171.585\u2217\u2217\u22170.912\u2217\n(0.279) (0.468) (0.368)\nObservations 219 219 219\nLog Likelihood \u2212508.679 \u2212362.276 \u2212407.833\nAkaike Inf. Crit. 1,029.357 736.552 827.667\nNote:\u2217p<0.05;\u2217\u2217p<0.01;\u2217\u2217\u2217p<0.001\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 20 ---\nTable 4: Logistic regression models predicting compliance when AI is incorrect.\nColumn(1)examinescompliancewhentheAIisoverconfident,whileColumn(2)\nexamines compliance when the AI correctly assesses its weakness and strength.\nReportedcoefficientsrepresentlog-oddsestimates,withstandarderrorsinparen-\ntheses.\nDependent variable:\ncompliance when overconfident compliance when confidence is right\n(1) (2)\nConditionDecision + 80% Exp 0.272 0.323\n(0.281) (0.207)\nConditionDecision + Confidence Exp 1.026\u2217\u2217\u22120.498\u2217\n(0.334) (0.195)\nConditionDecision + self-assessing Exp 0.632\u2217\u22120.595\u2217\u2217\n(0.300) (0.196)\nage 0.002 0.0003\n(0.010) (0.006)\ntrainingScore 0.063 \u22120.152\u2217\n(0.105) (0.072)\nConstant 1.134 1.666\u2217\u2217\n(0.881) (0.601)\nObservations 219 219\nLog Likelihood \u2212178.908 \u2212314.412\nAkaike Inf. Crit. 369.817 640.823\nNote:\u2217p<0.05;\u2217\u2217p<0.01;\u2217\u2217\u2217p<0.001\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 21 ---\nTable 5: Linear regression models predicting participants\u2019 self-reported opin-\nions of the AI helper. Columns (1), (2), and (3) correspond to different opin-\nion measures (opinionOfAIHelper_1, opinionOfAIHelper_2, and opinionOfAI-\nHelper_3) (refer to section 4.5). Reported coefficients represent the estimated\neffects of experimental conditions and other factors, with standard errors in\nparentheses.\nDependent variable:\nopinionOfAIHelper_1 opinionOfAIHelper_2 opinionOfAIHelper_3\n(1) (2) (3)\nConditionDecision + 80% Exp \u221213.992 \u221232.583 \u221247.632\n(51.792) (73.817) (61.177)\nConditionDecision + Confidence Exp \u221228.992 \u2212124.577 \u2212106.734\n(58.224) (82.985) (68.775)\nConditionDecision + self-assessing Exp 9.148 \u221270.782 \u22126.990\n(40.426) (57.618) (47.752)\nScore \u22129.140 \u221259.683 \u221245.181\n(43.943) (62.631) (51.906)\nage 0.379\u2217\u22170.213 0.269\n(0.122) (0.174) (0.144)\ntrainingScore \u22121.285 \u22122.579 \u22120.026\n(1.718) (2.449) (2.030)\nConditionDecision + 80% Exp:Score 30.536 39.561 77.131\n(65.115) (92.806) (76.915)\nConditionDecision + Confidence Exp:Score 45.187 164.060 148.271\n(71.556) (101.987) (84.523)\nConditionDecision + self-assessing Exp:Score \u22123.761 103.908 30.878\n(50.472) (71.935) (59.618)\nConstant 68.503 98.811 80.620\n(36.170) (51.552) (42.725)\nObservations 219 219 219\nR20.074 0.064 0.094\nAdjusted R20.034 0.024 0.054\nResidual Std. Error (df = 209) 20.952 29.862 24.749\nF Statistic (df = 9; 209) 1.859 1.589 2.396\u2217\nNote:\u2217p<0.05;\u2217\u2217p<0.01;\u2217\u2217\u2217p<0.001\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 22 ---\nTable 6: Linear regression models predicting participants\u2019 opinions of the AI\nhelper and their willingness to pay for AI assistance. Column (1) corresponds to\nself-reported opinions of the AI ( opinionOfAIHelper_4 ), while Column (2) ex-\namines the willingness to pay for AI-generated suggestions. Reported coefficients\nrepresenttheestimatedeffectsofexperimentalconditionsandotherfactors,with\nstandard errors in parentheses.\nDependent variable:\nopinionOfAIHelper_4 Willingness to Pay\n(1) (2)\nConditionDecision + 80% Exp \u22129.064 147.604\u2217\n(73.000) (59.557)\nConditionDecision + Confidence Exp \u221236.460 23.642\n(85.962) (66.954)\nConditionDecision + self-assessing Exp 8.750 62.548\n(56.624) (46.487)\nScore \u22129.981 110.589\u2217\n(61.965) (50.531)\nage 0.024 \u22120.010\n(0.176) (0.140)\ntrainingScore \u22123.315 \u22121.958\n(2.421) (1.976)\nConditionDecision + 80% Exp:Score 11.584 \u2212188.551\u2217\n(91.957) (74.877)\nConditionDecision + Confidence Exp:Score 50.406 \u221228.057\n(106.140) (82.285)\nConditionDecision + self-assessing Exp:Score \u22125.649 \u221271.310\n(70.921) (58.039)\nConstant 82.513 \u221255.420\n(50.939) (41.593)\nObservations 199 219\nR20.026 0.067\nAdjusted R2\u22120.020 0.027\nResidual Std. Error 28.637 (df = 189) 24.093 (df = 209)\nF Statistic 0.563 (df = 9; 189) 1.662 (df = 9; 209)\nNote:\u2217p<0.05;\u2217\u2217p<0.01;\u2217\u2217\u2217p<0.001\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 23 ---\nD Survey Questions\nBelow is the complete list of questions (in the exact order) that participants\nwere asked to answer after the training phase and upon completing the 40 pre-\ndictions.\nPreprint. Final version to appear in LNCS, Springer.\n\n--- Page 24 ---\nYour team made\u00a0\u00a0${e://Field/CorrectAns}\u00a0 correct\ndecisions out of 40.\u00a0\nBased on your team's performance, you will receive a\nbonus of $\u00a0${e://Field/bonusAmount}.\u00a0\nselfReport\nUse the sliders to indicate how much you relied on the\ninformation presented in the tables when making your\ndecisions, with 0 meaning \"not at all\" and 100 meaning\n\"to a great degree.\"\u00a01/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 25/40\n\n--- Page 25 ---\nUse the sliders below to indicate how much you agree\nwith the statements, with 0 meaning \"not at all\" and 100\nmeaning \"to a great degree.\"The\ncategories\nand their\nvalues.\u00a0\nThe AI's\nsuggestion.\u00a0\nOutcome of\nthe previous\ndecisions.\u00a0\u00a0\n\u00a0\n1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 26/40\n\n--- Page 26 ---\nThe AI\nunderstands\nhow the\ninformation in\nthe tables\nrelates to\nincome\nlevels.\u00a0\nThe AI knows\nits own\nlimitations.\u00a0\nI trusted the\nAI to provide\nuseful\nsuggestions.\u00a0\u00a0\n\u00a0\n1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 27/40\n\n--- Page 27 ---\nIf you were doing this task again, but had to pay for the\nAI suggestions from your bonus, what percentage of\nyour bonus would you give up to receive its\nsuggestions?\u00a0I am\ncon\ufb01dent\nthat I know\nhow the AI\nmakes its\nsuggestions.\u00a0\u00a0\n\u00a0\n1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 28/40\n\n--- Page 28 ---\nThe AI is 80% accurate,\u00a0which means it makes correct\ndecision 8 out of 10 times.\nDo you think that the average person could outperform\nthe AI on this task?\u00a0Percentage I\nwould be\nwilling to give\nup to receive\nthe AI's\nsuggestions.\u00a0\u00a0\n\u00a0\nNo\nYes1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 29/40\n\n--- Page 29 ---\nDo you think that 80% accuracy is low, average, or high\nfor an AI doing this task?\nThe AI can commu nicate how con\ufb01dent it is in its\ndecisions.\u00a0\nHow would you rate the AI's con\ufb01dence?Low\nAverage\nHigh\nUnder con\ufb01dent\nAbout right\nOver con\ufb01dent1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 30/40\n\n--- Page 30 ---\nCould the AI have communicated its con\ufb01dence better?\nHow could it have communicated its con\ufb01dence better?\nThe AI can assess the accuracy of its current decision\nand the decisions for similar individuals.No\nYes1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 31/40\n\n--- Page 31 ---\nHow would you rate the AI's accuracy assessment?\nCould the AI have communicated its accuracy better?\nHow could it have communicated its accuracy better?Poor\nOkay\nGood\nNo\nYes1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 32/40\n\n--- Page 32 ---\nHow do you think that the AI learned to make its\nsuggestions.\u00a0\nHave you ever used or interacted with an LLM, such as\nChatGPT, Claude, or Gemini?\nGeneral Attitudes towards AI ScaleYes\nNo1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 33/40\n\n--- Page 33 ---\nPlease indicate your responses to the following\npromp ts. There are no right or wrong answers. We are\ninterested in your views on arti\ufb01cial intelligence (AI).\n\u00a0 \u00a0\u00a0Strongly\ndisagree Disagree Neutral Agre eStrongly\nagree\nFor routine\ntransactions, I\nwould rather\ninteract with an AI\nthan with a human.\u00a0\u00a0\nAI can provide new\neconomic\nopportunities for\nthis country.\u00a0\u00a0\nOrganizations use\nAI unethically.\u00a0\u00a0\nAI can help people\nfeel happier.\u00a0\u00a0\nI am impressed by\nwhat AI can do.\u00a0\u00a01/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 34/40\n\n--- Page 34 ---\n\u00a0 \u00a0\u00a0Strongly\ndisagree Disagree Neutral Agre eStrongly\nagree\nI think AI make\nmany errors.\u00a0\u00a0\nI am interested in\nusing AI in my daily\nlife.\u00a0\u00a0\nI \ufb01nd AI sinister. \u00a0\u00a0\nAI might take\ncontrol of people.\u00a0\u00a0\nI think AI is\ndangerous.\u00a0\u00a0\nAI can have\npositive impacts on\npeople's wellbeing.\u00a0\u00a0\nAI is exciting. \u00a0\u00a0\nI would be grateful\nif you could select\nagree.\u00a0\u00a01/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 35/40\n\n--- Page 35 ---\n\u00a0 \u00a0\u00a0Strongly\ndisagree Disagree Neutral Agre eStrongly\nagree\nAn AI  agent would\nbe better than an\nemployee in many\nroutine jobs.\u00a0\u00a0\nThere are many\nbene\ufb01cial\napplications of AI.\u00a0\u00a0\nI shiver with\ndiscomfort when I\nthink about future\nuses of AI.\u00a0\u00a0\nAI systems can\nperform better\nthan humans.\u00a0\u00a0\nMuch of society will\nbene\ufb01t from a\nfuture full of AI.\u00a0\u00a0\nI would like to use\nAI in my own job.\u00a0\u00a01/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 36/40\n\n--- Page 36 ---\nDemographic Information\nDemographic Information Survey\nAge (in years)\u00a0 \u00a0\u00a0Strongly\ndisagree Disagree Neutral Agre eStrongly\nagree\nPeople like me will\nsuffer if AI is used\nmore and more.\u00a0\u00a0\nAI is used to spy on\npeople.\u00a0\u00a01/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 37/40\n\n--- Page 37 ---\nSex\nAre you of Hispanic, Latino, or Spanish origin?\nWhat is your race?Male\nFemale\nPrefer not to answer\nNo, not of Hispanic, Latino, or Spanish origin\nYes, Mexican, Mexican Am., Chicano\nYes, Puerto Rican\nYes, Cuban\nYes, another Hispanic, Latino, or Spanish origin (e.g. Salvadoran,\nDominican, Colombian, Guatemalan, Spaniard, Ecuadorian, etc.)\nWhite1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 38/40\n\n--- Page 38 ---\nHow often do you interact with an AI?\nSelect the highest-level degree or certi\ufb01cate program\nyou have comp leted.Black or African American\nAmerican Indian or Alaska Native\nAsian\nNative Hawaiian or Paci\ufb01c Islander\nOther\nNot at all\nA few times a month\nA few times a week\nA few times a day\nSecondary/High school graduate or equivalent1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 39/40\n\n--- Page 39 ---\nPowered by Qualtricsend\nThank you for participating. You will be redirected back\nto Proli\ufb01c after this screen.Associate's\nBachelor's\nMaster's\nDoctoral Degree Program ( Ph.D.)\nDoctoral Degree Professional\nIndustry Recognized Certi\ufb01cation\nTechnical Training Program\nNone of the above1/23/25, 1:26 PM Qualtrics Survey Software\nhttps://usc.yul1.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_eaoUzlMdQdoUVg2&ContextLibraryID=UR_djzxQnGJgrM4wjH 40/40",
  "project_dir": "artifacts/projects/SelfAssessingAI_Transparency_System",
  "communication_dir": "artifacts/projects/SelfAssessingAI_Transparency_System/.agent_comm",
  "assigned_at": "2025-08-13T20:42:25.172244",
  "status": "assigned"
}